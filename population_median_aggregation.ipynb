{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "import seaborn as sb\n",
    "\n",
    "from IPython.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Data \n",
    "\n",
    "\n",
    "def remove_multiple_forecasts(tab_file_name, output_file):\n",
    "    \"\"\"\n",
    "    In: name of tab file, and name of file to return\n",
    "    Delete delimiter to allow for CSV files\n",
    "    Takes the latest submission by each contestant and writes to a csv file\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(tab_file_name, parse_dates=['timestamp'], delimiter = '\\t')\n",
    "    q_dat = data.groupby(by=['ifp_id','user_id']).apply(lambda x: x[x.timestamp == x.timestamp.max()])\n",
    "    q_dat.index = q_dat.index.droplevel().droplevel() #2 droplevels since grouping by 2\n",
    "    q_dat.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# tab_file = 'survey_fcasts.yr4.tab'\n",
    "# output_file = 'survey_fcasts.yr4.pandas.csv'\n",
    "\n",
    "# remove_multiple_forecasts(tab_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "def correct_answer(ifp_id):\n",
    "    # Given an IFP, return its correct answer\n",
    "    mask = IFP['ifp_id'] == ifp_id\n",
    "    corr = IFP['outcome'][mask]\n",
    "    return corr.values.item()\n",
    "\n",
    "def no_opts(ifp_id):\n",
    "    # Given an IFP, return the number of options associated\n",
    "    mask = IFP['ifp_id'] == ifp_id\n",
    "    no = IFP['n_opts'][mask]\n",
    "    return no.values.item()\n",
    "\n",
    "def compute_brier(prob, truth):\n",
    "    \"\"\"\n",
    "    Computes Brier scores according to \n",
    "    \n",
    "    Inputs:\n",
    "    prob: dims = n_options x 1\n",
    "    truth: dims = n_options x 1\n",
    "    \n",
    "    Outputs: Single value of Brier score for a particular question\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.shape(prob) != np.shape(truth):\n",
    "        Warning('Input dimensions must be consistent!')\n",
    "    \n",
    "    return np.sum((truth - prob)**2)\n",
    "\n",
    "def build_outcome_dict(csvfile):\n",
    "    \"\"\"This function takes a file (ifps.csv) and builds a dictionary mapping each question\n",
    "        to the outcome of that question (i.e. {1004-0:'a'})\n",
    "    \"\"\"\n",
    "    outcome_dict = {}\n",
    "    with open(csvfile, encoding = \"ISO-8859-1\") as file:  # encoding so python does not throw unicode error\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            outcome_dict[row[0]] = row[9]\n",
    "        outcome_dict.pop(\"ifp_id\", None)\n",
    "    print(\"outcome dict built\")\n",
    "    return outcome_dict\n",
    "\n",
    "def brier_scores(outcome_dict, median_dict):\n",
    "    \"\"\"Builds dictionary of brier scores for each question using two dicts as inputs\n",
    "    \"\"\"\n",
    "    brier_dict = {}\n",
    "    for question in median_dict.keys():\n",
    "        brier = 0\n",
    "        for response in median_dict[question].keys():\n",
    "            if response == outcome_dict[question]:\n",
    "                brier += (1.0 - median_dict[question][response] ) **2\n",
    "            elif response != outcome_dict[question]:\n",
    "                brier += (median_dict[question][response] ) **2\n",
    "        brier_dict[question] = brier\n",
    "        #print(question, \"added\")\n",
    "    return brier_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Reading in correct files\n",
    "\n",
    "forecast_file = 'survey_fcasts.yr2.latest_response.csv'\n",
    "\n",
    "IFP = pd.read_csv('ifps.csv', encoding='latin1')\n",
    "ifp_dict = build_outcome_dict('ifps.csv')\n",
    "data = pd.read_csv(forecast_file)\n",
    "\n",
    "unique_ifps = data['ifp_id'].unique()\n",
    "\n",
    "\n",
    "answer_choices = list(string.ascii_lowercase)\n",
    "df_of_answers = [] #this list will contain dataframes for every answer (e.g. 1001-0 a, 1001-0 b)\n",
    "\n",
    "for ifp in unique_ifps:\n",
    "    for i in range(no_opts(ifp)):   #this iterates through every choice that the given ifp has\n",
    "        letter = answer_choices[i]\n",
    "        df = data[(data['ifp_id'] == ifp) & (data['answer_option'] == letter)]\n",
    "        df_of_answers.append(df)\n",
    "\n",
    "\n",
    "median_outcomes = {}\n",
    "option_answers = {}\n",
    "for qa_pair in df_of_answers: # eg 1001a 1001b 1002a 1002b 1002c\n",
    "    \n",
    "    ifp_id = qa_pair['ifp_id'].unique()[0]\n",
    "    ans = qa_pair['answer_option'].unique()[0]\n",
    "    if ifp_id not in median_outcomes.keys():\n",
    "        option_answers = {ans : np.median(qa_pair['value'])}\n",
    "        median_outcomes[ifp_id] = option_answers\n",
    "    option_answers[ans] = np.median(qa_pair['value'])\n",
    "\n",
    "median_brier_dict = brier_scores(ifp_dict, median_outcomes)\n",
    "print('median_brier_dict built with',len(median_brier_dict.values()),'values')\n",
    "                  \n",
    "                  #stratify data set with different characteristics and see\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "### Population Code dictionary building\n",
    "\n",
    "def build_population_code_dictionary(data, removeUninformative=False):\n",
    "    \"\"\"\n",
    "    Data is in the form of a dataframe containing the survey_fcasts type file.\n",
    "    removeUninformative is intended to remove the very low confidence answers\n",
    "    Must have columns 'ifp_id', 'user_id', 'answer_option', 'value'\n",
    "    returns dictionary mapping scores to each question. \n",
    "    e.g. {'1001-0': {'a': 0.5, 'b': 0.5}, '1076-0': {'a': 0.12, 'b': 0.88}}\n",
    "    \"\"\"\n",
    "    population_outcomes = {}\n",
    "\n",
    "    for ifp_id, group in data.groupby('ifp_id'): \n",
    "        answer_dict = {}     # will be {'a': 144,'b': 232} format\n",
    "        for user_id, sub_group in group.groupby('user_id'):\n",
    "            normalised = sub_group['value'] / sub_group['value'].sum()\n",
    "            c = np.random.choice(list(sub_group['answer_option']),p=list(normalised)) # c = choice\n",
    "            if c not in answer_dict:\n",
    "                answer_dict[c] = 1\n",
    "            else:\n",
    "                answer_dict[c] += 1\n",
    "        population_outcomes[ifp_id] = answer_dict\n",
    "    for question in population_outcomes.keys():\n",
    "        total  = 0\n",
    "        for answer in population_outcomes[question].keys():\n",
    "            total += population_outcomes[question][answer]\n",
    "        for answer in population_outcomes[question].keys(): # used to be 'all_questions'\n",
    "            population_outcomes[question][answer] /= total\n",
    "    return population_outcomes\n",
    "\n",
    "pop_brier_dict = brier_scores(ifp_dict, population_outcomes)\n",
    "# pop_brier_dict is a dictionary mapping each question to a brier score\n",
    "# print('pop_brier_dict built with', len(pop_brier_dict.values()),'values')\n",
    "\n",
    "\n",
    "# ### Outcome\n",
    "# \n",
    "# Below are the brier scores for both the Population Code and simple Median Aggregation\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome\n",
    "\n",
    "Below are the brier scores for both the Population Code and simple Median Aggregation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(pop_brier_dict.items()))\n",
    "df=df.rename(columns = {0:'ifp_id',1:'Population Code'})\n",
    "# print(list(pop_brier_dict.values()))\n",
    "# df['Population Code'] = list(pop_brier_dict.values())\n",
    "df['Medians'] = list(median_brier_dict.values())\n",
    "\n",
    "df.head()\n",
    "print(df['Population Code'].mean())\n",
    "print(df['Medians'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the dataset of first year forecasts, the Population Code aggregation (mean brier = 0.223) is outperformed by the median aggregation (0.195).\n",
    "\n",
    "\n",
    "Next:\n",
    "    - Modify scripts to convert json to csv files\n",
    "    - Swapping forecast from earliest to latest\n",
    "    - Try population using mode method of aggregation\n",
    "    - Look at characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot = df.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Principal Component Analysis on matrix of n questions by m participants. \n",
    "First we will attempt analysis on binary questions using the confidence level given for option a.\n",
    "\n",
    "What will go in the matrix mapping of the question to the participants is the brier score for a given participant- response pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "forecast_file = 'survey_fcasts.yr1.latest_response.csv'\n",
    "\n",
    "IFP = pd.read_csv('ifps.csv', encoding='latin1')\n",
    "#ifp_dict = build_outcome_dict('ifps.csv')\n",
    "data = pd.read_csv(forecast_file)\n",
    "\n",
    "unique_ifps = data['ifp_id'].unique()\n",
    "unique_participants = data['user_id'].unique()\n",
    "\n",
    "\n",
    "#apply pca both on forecasts and on brier scores\n",
    "#try to fill values with mean or with baseline .50 scores\n",
    "#convert to json to use data_loader.py and then you can use the binary? column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis Data Preparation\n",
    "Here we are trying to create a matrix of user_id and ifp_id. The ifp_id has to be a binary answer. The values in the matrix will be the chance that each person thinks option a will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th>3.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>15.0</th>\n",
       "      <th>19.0</th>\n",
       "      <th>22.0</th>\n",
       "      <th>23.0</th>\n",
       "      <th>25.0</th>\n",
       "      <th>33.0</th>\n",
       "      <th>35.0</th>\n",
       "      <th>...</th>\n",
       "      <th>5903.0</th>\n",
       "      <th>5907.0</th>\n",
       "      <th>5908.0</th>\n",
       "      <th>5915.0</th>\n",
       "      <th>5918.0</th>\n",
       "      <th>5921.0</th>\n",
       "      <th>5924.0</th>\n",
       "      <th>5927.0</th>\n",
       "      <th>5928.0</th>\n",
       "      <th>5929.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_option</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ifp_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001-0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.171670</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002-0</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.141242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003-0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.176125</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004-0</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "      <td>0.242804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005-0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "      <td>0.740762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1972 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               value                                                         \\\n",
       "user_id       3.0       5.0    6.0    15.0   19.0      22.0   23.0   25.0     \n",
       "answer_option      a         a      a      a      a         a      a      a   \n",
       "ifp_id                                                                        \n",
       "1001-0          0.50  0.171670    0.0   0.25   0.60  0.171670   0.20    0.1   \n",
       "1002-0          0.33  0.141242    0.0   0.30   0.48  0.141242   0.75    0.2   \n",
       "1003-0          0.50  0.176125    0.0   0.50   0.15  0.100000   0.00    0.2   \n",
       "1004-0          0.65  0.242804    0.0   0.40   0.10  0.000000   0.00    0.7   \n",
       "1005-0          0.50  0.740762    1.0   0.70   0.75  0.740762   0.99    0.6   \n",
       "\n",
       "                                  ...                                   \\\n",
       "user_id          33.0   35.0      ...       5903.0    5907.0    5908.0   \n",
       "answer_option         a      a    ...            a         a         a   \n",
       "ifp_id                            ...                                    \n",
       "1001-0         0.200000   0.50    ...     0.100000  0.050000  0.020000   \n",
       "1002-0         0.350000   0.50    ...     0.141242  0.141242  0.000000   \n",
       "1003-0         0.176125   0.05    ...     0.350000  0.000000  0.000000   \n",
       "1004-0         0.242804   0.10    ...     0.242804  0.242804  0.242804   \n",
       "1005-0         0.740762   0.60    ...     0.740762  0.740762  0.740762   \n",
       "\n",
       "                                                                           \\\n",
       "user_id          5915.0    5918.0    5921.0    5924.0    5927.0    5928.0   \n",
       "answer_option         a         a         a         a         a         a   \n",
       "ifp_id                                                                      \n",
       "1001-0         0.171670  0.050000  0.171670  0.150000  0.000000  1.000000   \n",
       "1002-0         0.141242  0.141242  0.141242  0.000000  0.000000  0.000000   \n",
       "1003-0         0.176125  0.050000  0.176125  0.120000  0.000000  0.200000   \n",
       "1004-0         0.242804  0.242804  0.242804  0.242804  0.242804  0.242804   \n",
       "1005-0         0.740762  0.740762  0.740762  0.740762  0.740762  0.740762   \n",
       "\n",
       "                         \n",
       "user_id          5929.0  \n",
       "answer_option         a  \n",
       "ifp_id                   \n",
       "1001-0         0.000000  \n",
       "1002-0         0.000000  \n",
       "1003-0         0.000000  \n",
       "1004-0         0.242804  \n",
       "1005-0         0.740762  \n",
       "\n",
       "[5 rows x 1972 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head()\n",
    "# df = pd.DataFrame(data['ifp_id'])\n",
    "# for ifp in unique_ifps:\n",
    "\n",
    "data_pivot_all_values = pd.pivot_table(data,index=['ifp_id'],columns = [\"user_id\",\"answer_option\"], values = [\"value\"])\n",
    "\n",
    "\n",
    "# binary_data = data_pivot.loc[(data_pivot['value'] == 0.50)]\n",
    "# df.loc[df['column_name'].isin(some_values)]\n",
    "\n",
    "\n",
    "# Everything below here works to build the matrix \n",
    "data_1 = data.loc[data['answer_option'] == 'a']  \n",
    "\n",
    "\n",
    "data_pivot = pd.pivot_table(data_1,index=['ifp_id'],columns = [\"user_id\",\"answer_option\"], values = [\"value\"])#,aggfunc= lambda x : x)\n",
    "\n",
    "data_transposed = data_pivot[:100].transpose() #we want the ifp_id to be in the column so we can use the df.mean() function\n",
    "\n",
    "\n",
    "final_data = data_transposed.fillna(data_transposed.mean())\n",
    "\n",
    "final_data_transposed = final_data.transpose()\n",
    "def do_PCA(data):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA() #initiate a object of the class PCA\n",
    "    pca.fit_transform(data)\n",
    "    return pca\n",
    "\n",
    "pca = do_PCA(final_data_transposed)\n",
    "\n",
    "# final_data_transposed.head()\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.show()\n",
    "#this works but still has non binary options in it\n",
    "#the graph below indicates first component is agreement (explains 70% of variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1972)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_data_transposed.head()\n",
    "final_data_transposed.shape # ifp ids x features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 68.36  70.77  72.44  73.54  74.43  75.25  76.01  76.73  77.42  78.07\n",
      "  78.67  79.23  79.76  80.28  80.78  81.27  81.73  82.17  82.6   83.02\n",
      "  83.43  83.84  84.24  84.63  85.02  85.39  85.76  86.12  86.48  86.83\n",
      "  87.17  87.51  87.84  88.16  88.47  88.78  89.08  89.37  89.66  89.94\n",
      "  90.22  90.49  90.76  91.03  91.29  91.55  91.8   92.05  92.3   92.54\n",
      "  92.77  93.    93.23  93.46  93.68  93.9   94.11  94.32  94.53  94.73\n",
      "  94.93  95.13  95.32  95.51  95.7   95.88  96.06  96.24  96.41  96.58\n",
      "  96.74  96.9   97.06  97.22  97.37  97.52  97.67  97.81  97.95  98.09\n",
      "  98.22  98.35  98.47  98.59  98.71  98.83  98.95  99.06  99.17  99.27\n",
      "  99.37  99.47  99.56  99.65  99.73  99.8   99.87  99.93  99.98  99.98]\n"
     ]
    }
   ],
   "source": [
    "# Below is the variance explained by each of the principal componenets (in cumulative % terms)\n",
    "print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)) \n",
    "# big first component indicates large amount of redundant information that can be collapsed into one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_pivot_filled = data_pivot.fillna(value=new_question_mean_dict,axis=0) #to replace NaN in rows with mean of the row\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pca = do_PCA(data_pivot[:50]) #\n",
    "# print(pca.explained_variance_ratio_)\n",
    "\n",
    "# #Try using preformatted json?\n",
    "# IFP_json = pd.read_json('ifps.json')\n",
    "# IFP_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE DATA STILL HAS SOME NON BINARY IFPS?\n",
    "\n",
    "# from data_loader import load_data\n",
    "# IFP_json = pd.read_json('ifps.json')\n",
    "# load_data('ifps.json','forecasts.yr1.json','--old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00884959  0.02234152  0.03131801 ...,  0.03116568  0.01707274\n",
      "  0.02709123]\n"
     ]
    }
   ],
   "source": [
    "print(pca.components_[0]) #this first component explains ~ 70% of variance\n",
    "# pca.components_[0] will be 1972x1 in size for each user_id\n",
    "#why is mean not 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Principal Component Logistic Regression\n",
    "\n",
    "Here we are aiming to use the principal components for logistic regression.\n",
    "We'll train the set of weights using the past data as weights. \n",
    "Input to the function will be the set of everybody's prediction for a given question. Output will be overall probability for question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "IFP = pd.read_csv('ifps.csv', encoding='latin1')\n",
    "\n",
    "# test_data = final_data_transposed.sample(frac = 0.10)\n",
    "train, test = train_test_split(final_data_transposed, test_size = 0.1)\n",
    "\n",
    "def build_target_list(df):\n",
    "    \"\"\"input: dataframe with ifp values.\n",
    "        creates list of [1,0,1,0,0...] corresponding to target values\n",
    "        \"\"\"\n",
    "    targets = []\n",
    "    for ifp in df.index:\n",
    "        result = IFP.loc[IFP['ifp_id'] == ifp]\n",
    "        value = result['outcome'] == 'a'\n",
    "        if value.item():\n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "    return targets\n",
    "\n",
    "total_targets = build_target_list(final_data_transposed)\n",
    "\n",
    "test_targets = build_target_list(test) \n",
    "train_targets = build_target_list(train)\n",
    "\n",
    "# now need to specify targets using vector of ifp_id results\n",
    "# are columns and rows around the right way now? yes: features are people and ifp_ids are samples\n",
    "\n",
    "#To do:\n",
    "# read documentation on sklearn LogisticRegression\n",
    "# train logistic regression model on all of data\n",
    "# use PCA weights to modify dataset (how??)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909387997623 0.0657378450129\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE\n",
    "# >>> from sklearn import datasets, linear_model\n",
    "# >>> from sklearn.model_selection import cross_val_score\n",
    "# >>> diabetes = datasets.load_diabetes()\n",
    "# >>> X = diabetes.data[:150]\n",
    "# >>> y = diabetes.target[:150]\n",
    "# >>> lasso = linear_model.Lasso()\n",
    "# >>> print(cross_val_score(lasso, X, y))  \n",
    "\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "scores = cross_val_score(log_reg, final_data_transposed, total_targets)\n",
    "print(scores.mean(), scores.std())\n",
    "# print(cross_val_score(log_reg, final_data_transposed, total_targets))\n",
    "\n",
    "# log_reg.fit(final_data_transposed, total_targets) #weights = PCA vectors? dimensions wrong??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident, using the cross_val_score function, we get relatively high validation scores on the data alone:\n",
    "\n",
    "[ 0.97058824  0.93939394  0.81818182]\n",
    "\n",
    "Next we will try using a dataframe consisting of the first couple PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750148544266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "contestants = []\n",
    "for tuple in final_data_transposed.columns.values:\n",
    "    contestants.append(tuple[1])\n",
    "\n",
    "pca_df = pd.DataFrame(columns = contestants) # len 1972\n",
    "for i in range(len(pca.components_)):\n",
    "     pca_df.loc[i] = pca.components_[i]\n",
    "pca_df.head()\n",
    "pca_scores = cross_val_score(log_reg, pca_df, total_targets)\n",
    "print(pca_scores.mean())\n",
    "#pca seems to do a much worse job:\n",
    "# %75.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-120-0618da1eecbe>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-120-0618da1eecbe>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    score = -1*cross_validationscore = -1*cross_validation.cross_val_score(log_reg, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='mean_squared_error').mean()\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## Here is attempt at Cross Validation\n",
    "n = 100 # not sure if this should be no. ifps(100) or no. individuals(1972)\n",
    "kf_10 = cross_validation.KFold(n,n_folds=10,shuffle=True,random_state=2)\n",
    "mean_squared_errors = []\n",
    "\n",
    "score = -1*cross_validationscore = -1*cross_validation.cross_val_score(log_reg, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='mean_squared_error').mean()    \n",
    "mse.append(score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we see briefly if the predictions by the logistic regression model\n",
    "are correct.\n",
    "We take the first row of the train set, and then see simply if the prediction matches\n",
    "the actual outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  [0]\n",
      "actual:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxlangenkamp/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "prediction = log_reg.predict(train.iloc[0])\n",
    "print('prediction: ',prediction)\n",
    "result = IFP.loc[IFP['ifp_id'] == '1017-0']\n",
    "value = result['outcome'] == 'a'\n",
    "print('actual: ',value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922222222222\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics, cross_validation\n",
    "predicted = cross_validation.cross_val_predict(LogisticRegression(), train, train_targets, cv=10)\n",
    "print (metrics.accuracy_score(train_targets, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
